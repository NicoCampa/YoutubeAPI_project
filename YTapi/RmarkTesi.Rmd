---
title: "Text mining analysis of Spotify app reviews on the Google Play Store"
author: "Nicolò Campagnoli"
date: "9/16/2022"
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
mainfont: Arial
fontsize: 12pt
geometry: left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm
---
\newpage
# **INDEX**  
**0. Abstract**   
**1. Introduction**  
1.1 Google Play Store  
→*1.1.1 Spotify*  
1.2 Text Mining   
**2. Methodology**  
2.1 The Dataset  
2.2 Cleaning the data  
→*2.2.1 Corpus*  
→*2.2.2 TDM - term document matrix*  
2.3 Exploratory data analysis  
→*2.3.1 Correlation between scores and length of a review*  
→*2.3.2 Word frequencies*  
2.4 Sentiment Analysis  
→*2.4.1 Bing lexicon*  
→*2.4.2 NRC lexicon*  
2.5 Word association  
2.6 Bi-grams  
→*2.6.1 Negation words in Bi-grams*  
2.7 Correlation  
2.8 qdap polarity  
→*2.8.1 TFIDF*  
**3. Conlusions**  
\newpage

# **0.  Abstract**

Spotify is the most famous audio streaming provider in the world, it has currently 433 million monthly active users. The main focus of this analysis is to go through the latest reviews of its app in the Google Play store, via means of text mining analysis, to capture as many useful insights as possible regarding possible complaints or strengths of the service.  

\newpage

# **1. Introduction**  
'A review is an evaluation of a publication, product, service, or company or a critical take on current affairs in literature, politics, or culture. In addition to a critical evaluation, the review's author may assign the work a rating to indicate its relative merit.' This is what Wikipedia has to say about reviews. Nowadays reviews are everywhere and easy to access basically in every online shop, or even via YouTube videos and other sources. They can influence customers' decisions whether to buy one product or to go to a restaurant far away with the respect to a closer one that has a lower rating on Google Maps or TripAdvisor. Reviews are useful to avoid fraud, but at the same time, we have to be aware that some of them could possibly be fake or biased. Sometimes the number of reviews is so huge that it's impossible to read them all and conclude something. This is the reason why we use Text Mining, which has the advantage to require little to no sampling. Bad reviews are commonly more important than good reviews because they are the only way to understand critical aspects and improve. This is why getting to know the reasons behind them is important and we're going to see possible ways to do it.   

### **1.1 Google Play Store**  
An app store refers to an online shop where customers can purchase and download various software applications. They can be accessed via the free client software or a web browser.
Google Play Store serves as the official app store for devices running Android OS and its derivatives, allowing users to browse and download applications either freely or at a cost. It originated from Android Market, which has been re-branded as Google Play Store in 2012. Other than apps, it's also a digital media store, offering a product like music, books, and movies.
Around 97% of all apps are available for free in the Play Store, the remaining are at a cost, but the majority cost less than a dollar. Given that, the largest proportion of revenue comes from solutions like in-app purchases or ads.
In-app purchases are divided into categories: consumable (like gems in a game), non-consumable (premium features purchased once), and subscriptions (they generally provide access to content).
In-app advertising is integrated via banners, or video and often they give consumable rewards. 
 

### **1.1.1 Spotify** 
Spotify is a freemium service, basic features are free with advertisements and limited control, while additional features, such as offline listening and commercial-free listening, are offered via paid subscriptions. It generates revenue by selling premium streaming subscriptions to users and advertising placements to third parties. Some of the premium options users may choose from include 'individual', 'duo', 'family', and 'student'. They all come at different prices. 'Student' is just a discounted individual account promo reserved for students, indeed. 
The main differences between a premium account and a free account are:   
• Ads  
• Music quality  
• Offline Music  
• Only shuffle mode (no individual selection of songs)  
• Limited number of skips (a song) per hour  
We have to keep them in mind because we will find them again in our analysis.   


### **1.2 Text Mining**   
Text mining is the process of extracting useful insights from text, a type of unstructured data. Essentially, it means going from an unorganized state to a summarized and structured state. During the analysis, we will see the 'bag of words' approach, in which every word or group of words, is treated as a unique feature of the reviews. In that way, we're not taking into account the order and the grammatical structure of the sentence, but we're using a very fast technique that is not computationally expensive.    
\newpage

# **2. Methodology** 

### **2.1 The Dataset**  
The dataset in analysis contains the latest 10,000 reviews of the Spotify app on the Google Play Store. It's characterized by 3 columns:   

• reviewID → _contains a unique identifier of the specific review_  
• content → _the text review itself_  
• score → _an integer variable ranging from 1 to 5, respectively worst and best score_   

it's taken from Kaggle, which is an online community and has a big repository of datasets.  
It's updated every day, I extracted it on 20/9/2022.  
For the purpose of this analysis, another dataset has been downloaded, to address an issue with the translation of emoji into text that we'll see later:  

• emoji → _the emoji itself_  
• name → _text version of the emoji_  
 
Also downloaded from Kaggle.


### **2.2 Cleaning the data**   
```{r, echo=FALSE}
# GET THE DATA:

#data <- read.csv(file = 'dati/text mining/Instagram.csv',header = T) #original reviews with emoji
data <- read.csv(file = 'dati/text mining/Spotify.csv',header = T)
emoji <- read.csv(file = 'dati/emoji/emoji_df.csv', header = T) #list of emoji to translate emoji into text
emojiASCII<- read.csv(file = 'dati/emoji/emoji_df_ASCII.csv', header = T) #same list but emoji in ASCII format
dataASCII <- read.csv(file = 'dati/emoji/SpotifyASCII.csv', header = T)#reviews but emoji in ASCII

emojiASCII <- emojiASCII[,c(1,3)] #extract only useful columns
colnames(emojiASCII) <- c('emoji', 'name') #rename
```
After we have imported the data into our program, it's useful to see an example of a review:  
```{r}
cat(data$content[3])
cat(data$content[9])
```
In order to translate emojis into text, several tries have been made. Starting from a function 'replace_emoji()' of the package textclean. That function worked, but only for a fraction of the total emoji. Given that every year new ones come out, I had to translate them using the 'mgsub()' function. Which takes as input a pattern (the target emoji) and a replacement (the text analog), which have been taken from the emoji dataset described before, and updated with the latest release. Another issue came from the encoding of the files, in the latter, I had to translate everything into ASCII to have the best compatibility.   

```{r,echo=FALSE} 
#TRANSLATE EMOJI INTO TEXT
library(textclean)
pattern <- emojiASCII$emoji
replacements <- emojiASCII$name
te <- as.data.frame(mgsub(pattern = pattern,replacement =  replacements, x = dataASCII$content))
finalData <- as.data.frame(cbind(te$`mgsub(pattern = pattern, replacement = replacements, x = dataASCII$content)`, 
                                 data$score))
colnames(finalData) <- c('content', 'score')
verify <- as.data.frame(cbind(te$`mgsub(pattern = pattern, replacement = replacements, x = dataASCII$content)`,
                              data$content))
#create also a version without emoji

#new data without emojis
data2 <- read.csv(file = 'dati/emoji/SpotifyASCII.csv',header = T)

te <- as.data.frame(mgsub(pattern = pattern,replacement =  ' ', x = data2$content))
dataNOEMO <- as.data.frame(cbind(te$`mgsub(pattern = pattern, replacement = " ", x = data2$content)`, 
                                 data2$score))
colnames(dataNOEMO) <- c('content', 'score')
verify2 <- as.data.frame(cbind(te$`mgsub(pattern = pattern, replacement = " ", x = data2$content)`,
                              data$content))
#i remove all emoji 
```

### **2.2.1 Corpus**  

Once we've prepared our data, we're ready to organize them into a corpus. A corpus is a body of organized text that gives a structure to unstructured text, in fact, it's a list of the document (in our case reviews). Now we have to clean the text, going through steps like transforming every letter to lowercase, removing numbers, removing stopwords, removing white spaces, and punctuation. The cited 'stopwords' are common words that don't provide any useful insights.  
The cleaning process is done for standardization of the text and to reduce noise.  
```{r,echo=FALSE, message=FALSE, warning=FALSE}
#CLEANING THE CORPUS
library(tm)
library(stringr)
corpus <- Corpus(VectorSource(finalData$content))

corpus <- tm_map(corpus, content_transformer(tolower)) #all lowercase
corpus <- tm_map(corpus, removeNumbers) #remove numbers
corpus <- tm_map(corpus, removeWords, c(tm::stopwords('en'),'app')) #remove english stopwords 
corpus <- tm_map(corpus, content_transformer(str_replace_all), '-', ' ') #replace dashes with spaces
corpus <- tm_map(corpus, content_transformer(str_replace_all), '–', ' ')
corpus <- tm_map(corpus, removePunctuation) #remove punctuation
corpus <- tm_map(corpus, stripWhitespace) #remove white spaces

```
```{r, message=FALSE, warning=FALSE}
inspect(corpus[3])
```
  
### **2.2.2 TDM - term document matrix**  
The term document matrix is the analytical representation of a corpus. In TDM each column represents an individual review, while each row the words. There exist also the DTM (document term matrix), which is its transpose. The appearance of a word in a review can be coded in many ways, thus this matrix can be weighted in different manners. We will use TF - term frequencies which simply count the number of occurrences by word.  
In our bag of words approach, the matrix is what the analytics are based on. 
```{r,echo=FALSE, message=FALSE, warning=FALSE}
tdm <- TermDocumentMatrix(corpus) 
m <- as.matrix(tdm) #matrix
df <- data.frame(text = sapply(corpus, as.character), stringsAsFactors = FALSE) #data.frame
v <- sort(rowSums(m),decreasing=TRUE) #term frequencies
d <- data.frame(word = names(v),freq=v)
```
Our matrix has `r dim(m)[1]` rows and `r dim(m)[2]` columns. Meaning that after cleaning, `r dim(m)[1]` different words have remained in `r dim(m)[2]` reviews.    

### **2.3 Exploratory data analysis**  
Now that we have our data cleaned and prepared, we can summarize the main characteristics of the dataset. We can start by having a look at the distribution of scores:  

```{r,echo=F,fig.align="center",message=FALSE, warning=FALSE}
#distribution of grades
barplot(table(data$score),
        col = c('#CFDEE7','#92B4F4','#5E7CE2','#4472CA','#0A369D'),
        main = 'distribution of scores') #distribution of grades
```
• 1 star reviews and 5 star reviews accounts for the `r (table(data$score)[1] + table(data$score)[5])/10000*100`% of the total. The mean score is `r round(mean(data$score),1)` stars.

### **2.3.1 Correlation between scores and length of a review**  
It's interesting to see how the length of a review measured in the number of characters could have an influence on the score. We'll see that by means of Ordinary Least Squares regression:    
\begin{align*}
y = \beta_{0} + \beta_{1} x + \varepsilon \\
\end{align*}
In our case the independent variable would be the scores.
```{r,echo=F,fig.align="center",message=FALSE, warning=FALSE, out.width = "80%"}
#nchar vs score
nchar <- c(mean(nchar(data[data$score == 1,2])),
           mean(nchar(data[data$score == 2,2])),
           mean(nchar(data[data$score == 3,2])),
           mean(nchar(data[data$score == 4,2])),
           mean(nchar(data[data$score == 5,2])))
scores <- c(1:5)
model <- lm(scores ~ nchar)

```

```{r,echo=F,fig.align="center",message=FALSE, warning=FALSE,out.width = "50%"}
plot(nchar,scores, ylab = 'scores', xlab = 'length of review')
abline(model$coefficients, col = 'blue', lwd= 2)
```
As we can see from the plot, the regression line has a negative slope, meaning that on average, on longer review correspond to a lower score.  

### **2.3.2 Word frequencies**   
Wordcloud is a popular and powerful tool for showing term frequencies.  
```{r,echo=F,fig.align="center",message=FALSE, warning=FALSE, out.width = "60%"}
#WORDCLOUD
library(wordcloud)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```
• The result sometimes could be crowded, so instead we can visualize the frequencies by a barplot:    
```{r,echo=F,fig.align="center",message=FALSE, warning=FALSE, out.width = "80%"}
#most frequent words
barplot(d[1:15,]$freq, las = 2, names.arg = d[1:15,]$word,
        col ="lightblue", main ="Most frequent words",
        ylab = "Word frequencies")
```
The emojis have a big impact on the frequencies as seen from words like 'heart', 'smiling', 'eyes', 'thumbs', and 'red'. This could be a sign that using emojis in the reviews on the Google Play store is a common behavior. Other words like 'ads', 'fix' and 'please' could be a sign of some sort of request regarding an issue. It's interesting to see 'premium' among the top 15 words, the paid service seems to be mentioned a lot in the reviews.  

### **2.4 Sentiment Analysis**  
Sentiment analysis is the process of extracting an author’s emotional intent from text. There are several emotional frameworks that can be used for sentiment analysis. We will focus on polarity, in which there are only two categories, positive and negative.  
Subjectivity lexicon assigns to a particular word an emotional state, there exists a few; we're going to see two of them:  
• Bing: categorizes words in a binary fashion into positive and negative categories.  
• Nrc: a list of English words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive).

### **2.4.1 Bing lexicon**  
We have to use tokenized words. Tokenization is feature extraction, the process of breaking up sequences of strings into pieces such as words in our case (for the moment).   
We apply sentiment polarity to tokenized words, by doing so we're able to plot the most frequent words by sentiment:   
```{r,echo=F,message=FALSE, warning=FALSE}
library(tidytext)
library(dplyr)
library(ggplot2)
#tokenization of cleaned corpus
wordTokens <- df %>% 
  unnest_tokens(word, text, drop = FALSE) %>% 
  select(-text) %>% 
  filter(!str_detect(word, '\\d')) %>% # delete words with numbers 
  filter(nchar(word) > 1) # delete words shorter than two letters

load(file = 'dati/lexicons/sentiments_lexicons.Rdata') #load lexicons

#apply sentiment polarity to words
wordSentiment <- wordTokens %>% 
  inner_join(bing, by = c('word' = 'word')) %>% 
  count(word, sentiment, sort = T) 

#aggregate them
word_sent_top <- wordSentiment %>% 
  group_by(sentiment) %>% 
  top_n(15, n) %>% 
  ungroup() %>% 
  mutate(word = reorder(word, n)) 

#plot them, most frequent words by sentiment in BING
ggplot(word_sent_top, aes(x = word, y = n, fill = sentiment)) +
  geom_col(show.legend = F) +
  facet_grid(~sentiment, scales = 'free_x') +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

• Given the distribution of the most common words for each sentiment (positive or negative) we can compute an overall score for the document based on the Bing lexicon:   
```{r,echo=F,message=FALSE, warning=FALSE}
library(tidyr)
data_sent <- wordTokens %>% 
  inner_join(bing) %>% 
  count(sentiment, sort = T) %>% 
  spread(sentiment, n) %>% 
  mutate(diff = positive - negative) 
scoreSentiment <- data.frame(t(data_sent))
colnames(scoreSentiment) <- 'value'
t(scoreSentiment)
```
The difference is greater than 0, meaning that if we consider the totality of the reviews, the prevailing sentiment is positive. It coherent with the average score of `r round(mean(data$score),1)` stars, closer to 5 instead of 1. 

### **2.4.2 NRC lexicon**  
We try also the NRC lexicon for comparison and to see if the results are consistent with respect to Bing. 

```{r,echo=F,message=FALSE, warning=FALSE}
nrc_polarity <- nrc %>% 
  filter(sentiment %in% c('positive', 'negative'))

word_sent2 <- wordTokens %>% 
  inner_join(nrc_polarity, by = c('word' = 'word')) %>% 
  count(word, sentiment, sort = T) 

word_sent_top <- word_sent2 %>% 
  group_by(sentiment) %>% 
  top_n(15, n) %>% 
  ungroup() %>% 
  mutate(word = reorder(word, n)) 
###most frequent words by sentiment in NRC
ggplot(word_sent_top, aes(x = word, y = n, fill = sentiment)) +
  geom_col(show.legend = F) +
  facet_grid(~sentiment, scales = 'free_x') +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

• Even though the two lexicons show some differences, the results are consistent. The most interesting words that need further investigation are words like 'annoying', 'problem', and 'hate'. There might be a possibility to find the causes of these reports.    

For the sake of curiosity we can also explore the value of the emotions of the NRC lexicon:   
```{r,echo=F,message=FALSE, warning=FALSE,  out.width = "80%"}
data_sent3 <- wordTokens %>% 
  inner_join(nrc) %>% 
  count(sentiment, sort = T) %>% 
  spread(sentiment, n)
scoreSentiment3 <- data.frame(t(data_sent3))
colnames(scoreSentiment3) <- c('value')
t(scoreSentiment3)
```
• Among emotions, joy is the most represent.  

### **2.5 Word association**  
We have seen in the sentiment analysis some words that could resemble an issue with the app. The next step is to explore which words are most associated with those ones. Word association is a sort of statistical correlation, but it ranges from 0 to 1 instead of -1 to 1. We now see the most associated words with 'problem' and 'annoying':  

```{r,echo=F,message=FALSE, warning=FALSE, out.width="50%"}

library(ggthemes)
associations<-findAssocs(tdm, 'problem', 0.12)
associations<-as.data.frame(associations)
associations$terms<-row.names(associations)
associations$terms<-factor(associations$terms,
                           levels=associations$terms)

ggplot(associations, aes(y=terms)) +
  geom_point(aes(x=problem), data=associations,
             size=5)+
  theme_gdocs()+ geom_text(aes(x=problem,
                               label=problem),
                           colour="darkred",hjust=-.25,size=8)+
  theme(text=element_text(size=20),
        axis.title.y=element_blank())

associations<-findAssocs(tdm, 'hate', 0.11)
associations<-as.data.frame(associations)
associations$terms<-row.names(associations)
associations$terms<-factor(associations$terms,
                           levels=associations$terms)

ggplot(associations, aes(y=terms)) +
  geom_point(aes(x=hate), data=associations,
             size=5)+
  theme_gdocs()+ geom_text(aes(x=hate,
                               label=hate),
                           colour="darkred",hjust=-.25,size=8)+
  theme(text=element_text(size=20),
        axis.title.y=element_blank())
```

• The presence of 'OTP' (= one-time password), 'network', 'logging', 'unlock', 'accessing', and so on, can be helpful in finding the most common bugs that occur in the customer's experience. At the same time, there are few terms useless for our purpose, this is because word association is not directly related to frequency but instead refers to the term pairings. We will see a proper correlation later.    

### **2.6 Bi-grams**  
Until now we have considered tokens of unigrams, which means just 1 word at a time. By examing how frequently one word is followed by another, we can build an idea on the most common topics. Another important aspect, is when a word is preceded by a negation like 'not', 'never', 'without'... etc etc that can change radically the sentiment on the review with respect to sentiment analysis on uni-grams like before. I decided to not consider emojis because the most frequent bi-grams in that would have been 'smiling-face', 'red-heart, and so on. In my opinion that is non-important and creates only noise in the following graphs. We start by having a look at the top 10 bi-grams by frequency:  

```{r,echo=F,message=FALSE, warning=FALSE, out.width="100%"}
#### advanced models####
### b-grams for sentiment analysis (no emoji)
library(igraph)
library(ggraph)
# data without emoji

corpusBi <- Corpus(VectorSource(dataNOEMO$content))

corpusBi <- tm_map(corpusBi, content_transformer(tolower)) #all lowercase
corpusBi <- tm_map(corpusBi, removeNumbers) #remove numbers
corpusBi <- tm_map(corpusBi, removeWords, c(tm::stopwords('en'),'app')) #remove english stopwords 
corpusBi <- tm_map(corpusBi, content_transformer(str_replace_all), '-', ' ') #replace dashes with spaces
corpusBi <- tm_map(corpusBi, content_transformer(str_replace_all), '–', ' ')
corpusBi <- tm_map(corpusBi, removePunctuation) #remove punctuation
corpusBi <- tm_map(corpusBi, stripWhitespace) #remove white spaces

dfBi <- data.frame(text = sapply(corpusBi, as.character), stringsAsFactors = FALSE)

bigrams <- dfBi %>% 
  unnest_tokens(bigram, text, token = 'ngrams', n = 2)  %>% 
  separate(bigram, c('c1', 'c2'), sep = " " )

#most frequent bigrams
bigram_counts <- bigrams %>% 
  count(c1, c2, sort = TRUE) #the emoji play a big role in that...
#i'll see how to adress it..

bigram_counts <- bigram_counts %>%
  drop_na(c1) %>%
  drop_na(c2)
head(bigram_counts, 10)
```
• It seems to rotate all around 'music', which is expected given the nature of the app.  
We can arrange words into a network connected by nodes using the package 'igraph'. Most common words will create center nodes. The shade of darkness of the arrow will depend on the frequency of that bi-gram. We now visualize only bi-grams with a frequency larger than 35:   

```{r warning=FALSE, out.width="100%", echo=FALSE, message=FALSE}
bigram_graph <- bigram_counts %>% 
  filter(n > 35) %>% # chooses bigrams that occur more than 35 times
  graph_from_data_frame() 


set.seed(1) 
#graph of bigrams
a <- grid::arrow(type = 'closed', length = unit(.1, 'inches')) # we want to have arrow on the plot

set.seed(1)
ggraph(bigram_graph, layout = "fr") + # to apply a layout to a graph
  geom_edge_link(aes(edge_alpha = n), # to make links more or less transparent, based on how frequent they appear
                 show.legend = FALSE,
                 arrow = a, # we will have arrows on the plot
                 end_cap = circle(.07, 'inches')) + # arrows will end before the touching the node
  geom_node_point(color = "red", size = 1) + 
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) + # to add names to the plot
  theme_void()
```
• Music is a center node, some interesting connections are 'free-version','many-ads', 'easy-use' and 'please-fix'.  

### **2.6.1 Negation words in Bi-grams**  

Without context, some sentiments could be misinterpreted. The next plot shows which words are misclassified, we will have an idea of the magnitude of the error thanks to the AFINN lexicon, which assign a numerical score (larger than 0 for positive, vice-versa for negative) instead of being just binomial. The x-axis is a multiplication between the sentiment score of the AFINN and the number of occurrences:  
```{r,echo=F,message=FALSE, warning=FALSE, out.width="100%"}

#see which words are preceded by a negation
negation_words <- c('not', 'no', 'never', 'nobody', 'without', 'nothing') 
negated_words <- bigrams %>% 
  filter(c1 %in% negation_words) %>% 
  inner_join(afinn, by = c('c2' = 'word')) %>% 
  count(c1, c2, value, sort = TRUE) %>% 
  ungroup


top_neg_word <- negated_words %>% 
  mutate(contribution = n * value) %>% 
  arrange(desc(abs(contribution))) %>% 
  group_by(c1) %>% 
  top_n(10, abs(contribution)) %>% 
  ungroup() %>% 
  mutate(c2 = reorder(c2, contribution)) 
#plotting the frequences
ggplot(top_neg_word, aes(c2, n * value, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  xlab("Words preceded by negation") +
  ylab("Sentiment score * number of occurrences") +
  facet_wrap(~c1, ncol = 2, scales = 'free') + 
  coord_flip()


```
• The misclassification with the biggest magnitude comes from the 'problem'. As we can see, other words are present but overall, the sizes are not that huge. We can be satisfied with the sentiment analysis performed before, as it is pretty accurate given that more than 8 thousand words have been categorized.   


### **2.7 Correlation**  
We are interested in words that co-occur within particular reviews or among them, even if not next to each other. We consider groups of 10 reviews each, and we're going to see which words are pairwise correlated using the pairwise_cor() function of the widyr package. We still use the data without emoji for the same reason of bi-grams. For the moment we focus on negative words that have come out from the sentiment analysis, to see whether we can find correlated topics:  

```{r,echo=F,message=FALSE, warning=FALSE, out.width="100%"}
### **2.7 Correlation** 
library(widyr)
#data without emojis
corpus2 <- Corpus(VectorSource(dataNOEMO$content))

corpus2 <- tm_map(corpus2, content_transformer(tolower)) #all lowercase
corpus2 <- tm_map(corpus2, removeNumbers) #remove numbers
corpus2 <- tm_map(corpus2, removeWords, c(tm::stopwords('en'),'app')) #remove english stopwords 
corpus2 <- tm_map(corpus2, content_transformer(str_replace_all), '-', ' ') #replace dashes with spaces
corpus2 <- tm_map(corpus2, content_transformer(str_replace_all), '–', ' ')
corpus2 <- tm_map(corpus2, removePunctuation) #remove punctuation
corpus2 <- tm_map(corpus2, stripWhitespace) #remove white spaces

df2 <- data.frame(text = sapply(corpus2, as.character), stringsAsFactors = FALSE)


df2$review <- rep(c(1:1000),each=10)
t <- df2 %>% 
  unnest_tokens(word, text, drop = FALSE) %>% 
  select(-text) %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, review, sort = TRUE) 

 
  


# We can pick some interesting words and find other most associated with them:
word_cors_top6 <- t %>%
  filter(item1 %in% c("annoying", "problem", "hate", "bad")) %>% # we choose those words
  group_by(item1) %>% 
  top_n(10) %>% # we choose top 10 (frequent) words 
  ungroup() %>%
  mutate(item2 = reorder(item2, correlation)) # new variable, that arranges second word acording to correlation
```
  

```{r,echo=F,message=FALSE, warning=FALSE, out.width="100%"}

# graph With colors:
ggplot(word_cors_top6, aes(item2, correlation, fill = correlation)) + 
  geom_col() + 
  facet_wrap(~ item1, scales = "free") + # to organize plots
  coord_flip() # flips orientation
```
• 'ads', 'want+skip', 'random', and 'recommendations' seems all to recall feature of the free version of Spotify as said in the introduction. Some people have a bad experience caused by 'login', 'network', and 'fix', probable bugs in the user experience.  

Similarly to n-grams we can build a network of words:
```{r,echo=F,message=FALSE, warning=FALSE, out.width="100%"}


# We can now visualize the correlations and clusters of words:
set.seed(2016)

# Let's use correlation matrix for network analysis:
t %>%
  filter(correlation > .20) %>% # filters data 
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(show.legend = FALSE) +
  geom_node_point(color = "#277BC0", size = 4) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()

# You can see that the output is rather symmetrical and there are no arrows - because
# the relationship isn't directional. 
# common (like in the bigram analysis earlier)
```

• In this case, the output is symmetrical because the correlation isn't directional.  

### 2.8 **qdap polarity**


Inside the qdap package we can find the polarity function, which is an algorithm that proceeds in the following manner:

1. it scans for positive and negative words within a subjectivity lexicon.
2. Once a polarity word is found, the function creates a cluster of terms, including the four preceding words and two following words.
3. Within the cluster, neutral words are counted as zero. The positive and negative words that form the basis of the cluster are counted as one and negative respectively. The remaining non-neutral and non-polarity words are therefore considered valence shifters. These valence shifters are given a weight to amplify or detract from the original polar word. The default value is 0.8. So amplifiers add 0.8 while negating words subtract 0.8.
4. All of the values in the word cluster are summed to create a total of the polarity with amplification or negation effects.
5. The grand total of positive, negative, amplifying, and negating words with their specific weights is then divided by the square root of all words in the passage. This helps to measure the density of the keywords.

If we consider an example:

_'Spotify is a very good app'_

• Positive = _'good'_  → +1  
• Neutral = _'Spotify'_, _'a'_, _'app'_  → 0  
• Valence shifter = _'very'_  → +0.8  

The sum is 1.8, which has to be divided by the square root of the number of words. 

$\frac{1.8}{\sqrt(6)}  = 0.73$  is the value of the polarity function for this sentence. 

• Now we can compute the polarity of each review in our dataset, an visualize the distribution:  

```{r,echo=F,message=FALSE, warning=FALSE, out.width="60%", fig.align='center'}
### 2.8 **qdap polarity**
library(qdap)
 
#### polarity
pol<-polarity(finalData$content)
ggplot(pol$all, aes(x=polarity ,
                        y=..density..)) + theme_gdocs() +
  geom_histogram(binwidth=.25,
                 fill="darkred",colour="grey60", size=.2) +
  geom_density(size=.75)

data5 <- finalData
data5$polarity <- scale(pol$all$polarity)
pos.comments<-subset(data5$content,
                     data5$polarity>0)
neg.comments<-subset(data5$content,
                     data5$polarity<0)

pos.terms<-paste(pos.comments,collapse = " ")
neg.terms<-paste(neg.comments,collapse = " ")
all.terms<-c(pos.terms,neg.terms)
all.corpus<-VCorpus(VectorSource(all.terms))


all.tdm<-TermDocumentMatrix(all.corpus, control=list(weighting=weightTfIdf, removePunctuation = TRUE,stopwords=stopwords(kind='en')))

all.tdm.m<-as.matrix(all.tdm)
colnames(all.tdm.m)<-c('positive','negative')
```

• The distribution isn't centered around zero, it's skewed to the left.
To build a comparison worldcould between positive and negative, we first need to scale the distribution around 0 to address the bias of 'grade inflation', in which people tend to mix some positivity in negative reviews. We will anyway see both comparison clouds to have an idea of the effect of scaling.

### 2.8.1 **TFIDF**

To create a comparison cloud we need two subsets of the original data: positive and negative reviews. The drawback of this approach is the elimination of all the neutral reviews with a score of 0. We will create one corpus containing two elements, one for each collapsed vector of characters containing either one of the polarities. From the corpus, we will obtain a TDM changing the term weights. The weighting has been changed to 'Term Frequency Inverse Document Frequency' (TFIDF).   
Instead of simple term frequency, the TFIDF value increases with the term occurrence but is offset by the overall frequency of the word in the corpus. The offsetting effect helps remove commonly occurring terms that may not yield much information. From a common sense perspective, if a term appears often, it must be important and represented in frequency. However, if it appears in all reviews, it is likely not that insightful or informational.  
It's calculated as the product of 'Term Frequency' (TF) and 'Inverse Document Frequency' (IDF), the latter is equal to _log(total document in corpus / # with term t in it)_.  

• Now we can build the comparison cloud based on scaled polarity score and weighted TFIDF:  


```{r,echo=F,message=FALSE, warning=FALSE, fig.align='center',out.width="80%"}
comparison.cloud(all.tdm.m, max.words=100,
                 colors=c('darkgreen','darkred'))
```
• And also the comparison cloud is still based on polarity score and TFIDF, but without scaling:    

```{r,echo=F,message=FALSE, warning=FALSE, fig.align='center',out.width="80%"}
data6 <- finalData
data6$polarity <- (pol$all$polarity)
pos.comments2<-subset(data5$content,
                     data5$polarity>0)
neg.comments2<-subset(data5$content,
                     data5$polarity<0)

pos.terms2<-paste(pos.comments2,collapse = " ")
neg.terms2<-paste(neg.comments2,collapse = " ")
all.terms2<-c(pos.terms2,neg.terms)
all.corpus2<-VCorpus(VectorSource(all.terms2))


all.tdm2<-TermDocumentMatrix(all.corpus2, control=list(weighting=weightTfIdf, removePunctuation = TRUE,stopwords=stopwords(kind='en')))

all.tdm.m2<-as.matrix(all.tdm2)
colnames(all.tdm.m2)<-c('positive','negative')
comparison.cloud(all.tdm.m2, max.words=100,
                 colors=c('darkgreen','darkred'))
```

\newpage

# **3. Conclusions**  

The biggest critical point of our analysis was the use of emojis. I jumped back and forth in using them depending on the type of analysis. They have been included in the sentiment analysis to capture as much polarity as possible but excluded from n-grams and correlation because their inclusion would have led to noisy results.   
We have captured some interesting insights:  
• On average, people that give a low score, tend to write more in the review.  
• The distribution of scores is bimodal, extreme values like 1 or 5 are the majority.  
• The sentiment analysis with Bing and NRC before, and then with the qdap polarity, are consistent with the average score. Overall, it's pretty positive.  
• some words might be misclassified in terms of polarity, for the interaction with negations for example. We have seen in our analysis that it happened, fortunately, for a very small portion.  
• As anyone could predict, the most discussed topic is music, as seen in Bi-grams.  
• From correlation and association we can conclude that people that give bad reviews can be divided into 2 categories:  
- People that aren't satisfied with the limitations of the free version (like ads or the number of skips per hour).  
- People lament some technical problems, like some issues with the network or logging into the app.  
• People that give good reviews talk about the interface being user-friendly, the app being easy to use, and some compliments about the music.  

It is to notice that even thou the base version has a lower quality of streaming music (capped at 160kbit/s compared to premium at 320kbit/s), nobody seems to complain, maybe regular users aren't aware about it, or they just can't hear the difference.  

In conclusion, Spotify has an overall good reception from the users, and the majority are satisfied. To convince disappointed customers in changing their idea, they could give more features to the free version and work on the reported issues. The first hypothesis is unlikely to happen, the resulting factor would be a premium version less appealing. Maybe the free version could unlock some new gimmicks only if the premium version gets as well some new updates.  


